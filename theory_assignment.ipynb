{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets focus on linear regression of the form \n",
    "\n",
    "$\\mathbf{y} \\approx f(\\mathbf{X}) = \\mathbf{X}\\mathbf{w_1} + \\mathbf{w_0}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 What are the rows of $\\mathbf{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of X corresponds to the m observations that describe the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What are the columns of $\\mathbf{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of X corresponds to the n features that describe the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we write the equation above as\n",
    "\n",
    "$\\mathbf{y} \\approx \\mathbf{\\tilde{X}}\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 How does $\\mathbf{\\tilde{X}}$ look like in this case (i.e., how does the shape of the matrix change compared to $\\mathbf{X}$)? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dimension of the matrix change : 2 columns and m number of rows. The matrix has now 2 features and m observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning, we need a cost function. Two common choices are the mean-squared error (MSE, $\\mathcal{L}_2$), and the mean-absolute error (MAE, $\\mathcal{L}_1$)\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_2 &=& \\frac{1}{2N} \\sum_{i=1}^N \\left(y_i - f(x_i) \\right)^2 \\\\\n",
    "    \\mathcal{L}_1 &=& \\frac{1}{2N} \\sum_{i=1}^N \\left|y_i - f(x_i) \\right| \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 In the Jupyter notebook, write a Python function that computes these two cost functions given an error term $\\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{\\tilde{X}}\\mathbf{w}$ (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(error_vector):\n",
    "    N = len(error_vector)\n",
    "    squared_error_vector = [i**2 for i in error_vector]\n",
    "    mean_squared_error = 1/N *sum(squared_error_vector)\n",
    "    return mean_squared_error\n",
    "\n",
    "#print(mean_squared_error(np.array([1,-1,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(error_vector):\n",
    "    N = len(error_vector)\n",
    "    absolute_error_vector = [abs(i) for i in error_vector]\n",
    "    mean_absolute_error = 1/N*sum(absolute_error_vector)\n",
    "    return mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should run as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mean_squared_error(np.array([0,0,0]))\n",
    "> returns 0\n",
    "```\n",
    "\n",
    "```python\n",
    "mean_squared_error(np.array([1,1,1]))\n",
    "> returns 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What is the shape of these cost functions as a function of the error  (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-10,10,100) \n",
    "x_axis_list = [[float(x)] for x in x_axis]\n",
    "\n",
    "y_mae = [mean_absolute_error(x) for x in x_axis_list]\n",
    "y_mse = [mean_squared_error(x) for x in x_axis_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, y_mse, label='MSE')\n",
    "plt.plot(x_axis, y_mae, label='MAE')\n",
    "plt.xlabel('error term')\n",
    "plt.ylabel('cost function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7  Are both loss functions differentiable for all $\\boldsymbol{\\epsilon}$? (1 point) What implications does this have for gradient based optimization like gradient descent? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the mean square error function is differentiable for all **∈**, the mean absolute error function is a bit different. Indeed, this last function is not differentiable at \n",
    " **∈ = 0** point due to the presence of the absolute value term in its formula. This difference between the two functions can be seen on the graph plotted above (question 1.6). On this plot, one can observe that while the MSE shows a smooth transition from negative to positive slope, the MAE function slope transition is really sharp. Finally, this non differentiability of the MAE function will prevent the use of gradiant based optimization methods since this function gradiant is undefined when \n",
    " **∈ = 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Which loss function is more sensitive to outliers (1 point) and why (1 point)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square error function is more sensitive to outliers since it will amplify quadratically the given errors. As a consequence, large errors will stand out much more prominently in comparison with the rest of the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the columns of $\\mathbf{X}$ are linearly independent.\n",
    "As a refresher of linear algebra, recall when the linear system $\\mathbf{X}\\mathbf{w} = \\mathbf{y}$ has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 One unique solution (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system Xw=y has a unique solution when, for our matrix X with linearly independent columns, we have: Rank(X) = Rank(X|y). This is equivalent to admitting that our scaled and reduced matrix X must have one pivot per column and per row. Furthermore, the system must be consistent, i.e. there must exist a w such that Xw=y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 No solution (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system has no solution when our matrix X, which has linearly independent columns, has Rank(X)< Rank(X|y). In fact, having linearly independent columns, the reduced scaled matrix X will have one pivot per column, but if Rank(X)< Rank(X|y) there will not be one pivot per row in the X matrice and the augmented matrix X|y will have a row such that 0 ...0|constant which is not mathematically correct because 0 =! constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 An infinite number of solutions (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With linearly independent columns, there are no infinite solutions. A matrix with linearly independent columns admits at most one solution, because once scaled and reduced it has one pivot per column, i.e. no free variable that would allow an infinite number of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the weights, $\\mathbf{w}$ we have to solve\n",
    "\\begin{equation}\n",
    "    \\mathbf{w} = \\left(\\mathbf{\\tilde{X}}\\mathbf{\\tilde{X}}^T\\right)^{-1}\\mathbf{\\tilde{X}}^T\\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 In general, why can't we solve eq.~\\ref{eq:linear_reg} using $\\mathbf{y} = \\tilde{\\mathbf{X}}^{-1}\\textbf{w}$? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be issues with invertibility, e.g., if n (number of columns) > m (number of rows) in the matrices. When the matrix is square, we can simply use the inverse matrix by multiplying equation number 2 (which we want to solve) by the inverse matrix of X on each side. This gives us w. However, as in most cases our matrix is not square, we won't have an inverse matrix, so we'll have to use other methods with the transposed matrix of X to get an invertible matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5  What happens if some columns are linearly dependent? (1 point) What is the connection to feature selection? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some columns are now linearly dependent and the XX^T matrix is no longer reversible, we can no longer use the fromule obtained earlier to calculate the weigths.\n",
    "\n",
    "As its name suggests, feature selection enables us to select the most important and independent characteristics of our system so that, when put into matrix form, our data corresponds to a matrix with linearly independent columns. In this way, we will no longer have the problem of inverting our matrices to perform our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6  What is the shape of the parabola as a function of $a$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parabola(x, a):\n",
    "    return a * x ** 2\n",
    "\n",
    "\n",
    "x_axis_parabola = np.linspace(-10, 10, 100)\n",
    "\n",
    "a_values = [5, -1, 1]\n",
    "colors = ['blue', 'red', 'green']\n",
    "labels = ['a = 5', 'a = -1', 'a = 1']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "for a, color, label in zip(a_values, colors, labels):\n",
    "    plt.plot(x_axis_parabola, parabola(x_axis_parabola, a), label=label, color=color)\n",
    "\n",
    "\n",
    "plt.title('Tracé des paraboles pour différentes valeurs de a')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axhline(0, color='black', linewidth=0.5, ls='--')\n",
    "plt.axvline(0, color='black', linewidth=0.5, ls='--')\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.ylim(-10, 10)  \n",
    "plt.xlim(-10, 10)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the sign of a, the shape of the parabola will change. If a is positive, the parabola faces upwards whereas for a negative, the parabola faces downwards. Moreover, the value of a will impact the shape of the parabola : the greater the a, the smaller the curve width as we can see in the last graph when we replace a by 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Plot the approximation to the function for different order polynomials ($N \\in \\{1, 2, 16\\}$) and with different regularization strength ($\\lambda \\in \\{0, 10^{-3}, 10^{-2}, 1\\}$). What do you observe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, 1, 100) # some grid for us on the x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # the number of points we will sample from true_function\n",
    "degrees = [1, 2, 16] # the polynomial degrees we will test\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_function(X) + np.random.randn(n_samples) * 0.1 # add some scaled random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=1 and different values for alpha parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial degrees and regularization strengths to explore\n",
    "degree = 1\n",
    "alphas = [0, 0.001, 0.01, 1]\n",
    "\n",
    "# Plot the noisy data and the true function\n",
    "plt.scatter(X, y, label=\"Noisy samples\", color=\"blue\")\n",
    "plt.plot(X_test, true_function(X_test), label=\"True function\", color=\"red\", linewidth=2)\n",
    "for alpha in alphas:\n",
    "    model = Pipeline([\n",
    "        (\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"ridge_regression\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    # Fit the model and predict\n",
    "    model.fit(X[:, np.newaxis], y)\n",
    "    y_pred = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "    # Plot the result for this alpha\n",
    "    label = f\"Degree {degree}, Alpha {alpha}\"\n",
    "    plt.plot(X_test, y_pred, label=label)\n",
    "\n",
    "# Add plot labels and legend\n",
    "plt.title(f\"Polynomial Approximation (Degree {degree}) with Different Regularization\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "alphas = [0, 0.001, 0.01, 1]\n",
    "\n",
    "# Plot the noisy data and the true function\n",
    "plt.scatter(X, y, label=\"Noisy samples\", color=\"blue\")\n",
    "plt.plot(X_test, true_function(X_test), label=\"True function\", color=\"red\", linewidth=2)\n",
    "for alpha in alphas:\n",
    "    model = Pipeline([\n",
    "        (\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"ridge_regression\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    # Fit the model and predict\n",
    "    model.fit(X[:, np.newaxis], y)\n",
    "    y_pred = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "    # Plot the result for this alpha\n",
    "    label = f\"Degree {degree}, Alpha {alpha}\"\n",
    "    plt.plot(X_test, y_pred, label=label)\n",
    "\n",
    "# Add plot labels and legend\n",
    "plt.title(f\"Polynomial Approximation (Degree {degree}) with Different Regularization\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 16\n",
    "alphas = [0, 0.001, 0.01, 1]\n",
    "# Plot the noisy data and the true function\n",
    "plt.scatter(X, y, label=\"Noisy samples\", color=\"blue\")\n",
    "plt.plot(X_test, true_function(X_test), label=\"True function\", color=\"red\", linewidth=2)\n",
    "for alpha in alphas:\n",
    "    model = Pipeline([\n",
    "        (\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"ridge_regression\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    # Fit the model and predict\n",
    "    model.fit(X[:, np.newaxis], y)\n",
    "    y_pred = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "    # Plot the result for this alpha\n",
    "    label = f\"Degree {degree}, Alpha {alpha}\"\n",
    "    plt.plot(X_test, y_pred, label=label)\n",
    "\n",
    "# Add plot labels and legend\n",
    "plt.title(f\"Polynomial Approximation (Degree {degree}) with Different Regularization\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we notice that we have a problem regularizing the function to a degree of 16. This is probably due to the very large degree of the function we want to use to approximate our true function. To see the effect of regularizing to this polynomial degree, we'll plot curve by curve, the approximation function of the function for different values of alpha. We'll start with alpha = 0 and work down to alpha = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.001)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0.001\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.01)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0.01\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=1)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 1\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a polynomial of order one, the degree-one model is not good at approximating the real function, because of the shape of the real function (which is not a linear function). This type of function does not explain the data well: the model is too simple to be used. If we now try to use a degree of 2, the true function is not well approximated by the model either, even for a small value of alpha, the regularizing force. For a polynomial of degree 16, the model is really complex but approximates the true function well for a regularization parameter of 0.001 and 0.01. As the regularization strength increases, the smoother the model curve, the less complex the model, but the data are not well approximated. We can also see that for N=16 and alpha=1, the curve is really not well approximated. \n",
    "\n",
    "The higher the regularization strength, the lower the complexity of the model. As a result, the model becomes simpler but does not approximate the true function. As far as polynomial degree is concerned, the higher the degree, the greater the complexity of the model and the less smooth the curve. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 What do you observe if you change the number of samples from the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we compare the result of the regularization of the true function for a number of samples of **10** and of **100**. We keep **N=16** for the polynomial degree and **alpha=0.01** for the regularization parameter because it was at these parameters that the function was best approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def true_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "\n",
    "X_test = np.linspace(0, 1, 100)  \n",
    "sample_sizes = [10, 100]  #try two values of samples\n",
    "degree = 16  #Polynomial degree\n",
    "alpha = 0.01  #regularization parameter\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "   \n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_function(X) + np.random.randn(n_samples) * 0.1  \n",
    "\n",
    "   \n",
    "    plt.scatter(X, y, label=f'{n_samples} noisy samples')\n",
    "    plt.plot(X_test, true_function(X_test), c='y', label='true function')\n",
    "\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    ridge_regression = Ridge(alpha=alpha)\n",
    "    pipeline_ridge = Pipeline([\n",
    "        (\"polynomial_features\", polynomial_features),\n",
    "        (\"ridge_regression\", ridge_regression)\n",
    "    ])\n",
    "    \n",
    "    pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "    \n",
    "    \n",
    "    plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), \n",
    "             label=f\"Model (N={n_samples}, Degree={degree}, Alpha={alpha})\")\n",
    "\n",
    "\n",
    "plt.title(\"Polynomial Regression with Ridge Regularization\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we can compare the two graphs for N=16 for the case with the number of samples equals to 10 and equals to 100. We see that at 10 samples, the model doesn't fit the true function whereas at 100 samples, it is. Therefore, if we increase the number of samples, let's say 100 samples, the model will give us a better approximation to the true function, we will get a better fit as we can observe by comparision with when we had 10 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Why do we need a test set in machine learning? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a set of tests is used to verify model performance, to detect and confirm the model. We need them to evaluate the model. It enables us to assess the extent to which the model generalizes to new cases. This evaluation enables us to better estimate the model's performance in the real world, where it encounters new inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 If we need to optimize hyperparameters, do we use the test set to select the best hyperparameters? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that need to be fixed before training the model. The model mustn't see the hyperparameters during the test set. We use cross-valition method to get a score for each set of hyperparameters. We don't have access to it during the test error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
