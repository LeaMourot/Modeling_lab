{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets focus on linear regression of the form \n",
    "\n",
    "$\\mathbf{y} \\approx f(\\mathbf{X}) = \\mathbf{X}\\mathbf{w_1} + \\mathbf{w_0}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 What are the rows of $\\mathbf{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of X corresponds to the m observations that describe the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 What are the columns of $\\mathbf{X}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of X corresponds to the n features that describe the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we write the equation above as\n",
    "\n",
    "$\\mathbf{y} \\approx \\mathbf{\\tilde{X}}\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 How does $\\mathbf{\\tilde{X}}$ look like in this case (i.e., how does the shape of the matrix change compared to $\\mathbf{X}$)? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dimension of the matrix change : 2 columns and m number of rows. The matrix has now 2 features and m observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning, we need a cost function. Two common choices are the mean-squared error (MSE, $\\mathcal{L}_2$), and the mean-absolute error (MAE, $\\mathcal{L}_1$)\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_2 &=& \\frac{1}{2N} \\sum_{i=1}^N \\left(y_i - f(x_i) \\right)^2 \\\\\n",
    "    \\mathcal{L}_1 &=& \\frac{1}{2N} \\sum_{i=1}^N \\left|y_i - f(x_i) \\right| \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 In the Jupyter notebook, write a Python function that computes these two cost functions given an error term $\\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{\\tilde{X}}\\mathbf{w}$ (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(error_vector):\n",
    "    mean_squared_error = # fill your code\n",
    "    return mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(error_vector):\n",
    "    mean_absolute_error = # fill your code\n",
    "    return mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should run as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mean_squared_error(np.array([0,0,0]))\n",
    "> returns 0\n",
    "```\n",
    "\n",
    "```python\n",
    "mean_squared_error(np.array([1,1,1]))\n",
    "> returns 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What is the shape of these cost functions as a function of the error  (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-10,10,100) # change as you wish for your plot\n",
    "y_mae = [mean_absolute_error(x) for x in x_axis]\n",
    "y_mse = [mean_squared_error(x) for x in x_axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, y_mse, label='MSE')\n",
    "plt.plot(x_axis, y_mae, label='MAE')\n",
    "plt.xlabel('error term')\n",
    "plt.ylabel('cost function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7  Are both loss functions differentiable for all $\\boldsymbol{\\epsilon}$? (1 point) What implications does this have for gradient based optimization like gradient descent? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__< Your answer >__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Which loss function is more sensitive to outliers (1 point) and why (1 point)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the error is small, a square of the error will give us a smaller value. If we have outliers, the absolute error function will be more sensitive because of the absolute value than the squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the columns of $\\mathbf{X}$ are linearly independent.\n",
    "As a refresher of linear algebra, recall when the linear system $\\mathbf{X}\\mathbf{w} = \\mathbf{y}$ has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 One unique solution (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 No solution (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 An infinite number of solutions (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the weights, $\\mathbf{w}$ we have to solve\n",
    "\\begin{equation}\n",
    "    \\mathbf{w} = \\left(\\mathbf{\\tilde{X}}\\mathbf{\\tilde{X}}^T\\right)^{-1}\\mathbf{\\tilde{X}}^T\\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 In general, why can't we solve eq.~\\ref{eq:linear_reg} using $\\mathbf{y} = \\tilde{\\mathbf{X}}^{-1}\\textbf{w}$? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know if the system gives us a unique solution. There might be issues with invertibility, e.g., if p > n (only square matrices are)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5  What happens if some columns are linearly dependent? (1 point) What is the connection to feature selection? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of solutions will be infinite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6  What is the shape of the parabola as a function of $a$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(x, a = 1): \n",
    "    return a * x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis_parabola = np.linspace(-10, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis_parabola, parabola(x_axis_parabola))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(x, a = -1): \n",
    "    return a * x ** 2\n",
    "x_axis_parabola = np.linspace(-10, 10, 100)\n",
    "plt.plot(x_axis_parabola, parabola(x_axis_parabola))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the sign of a, the shape of the parbola will change. If a is positive, the parabola faces upwards whereas for a negative, the parabola faces downwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Plot the approximation to the function for different order polynomials ($N \\in \\{1, 2, 16\\}$) and with different regularization strength ($\\lambda \\in \\{0, 10^{-3}, 10^{-2}, 1\\}$). What do you observe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, 1, 100) # some grid for us on the x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # the number of points we will sample from true_function\n",
    "degrees = [1, 2, 16] # the polynomial degrees we will test\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_function(X) + np.random.randn(n_samples) * 0.1 # add some scaled random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will fit a polynomial regression, you need to fill the degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the result, you can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"model of degree \")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can investigate the effect of the regularization parameter $\\lambda$ (function parameter `alpha`), For this, you can use the following code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting you can reuse the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 1 with regularization strength\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.001)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 1 with regularization strength of 0.001\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.01)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 1 with regularization strength of 0.01\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=1)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 1 with regularization strength of 1)\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2,\n",
    "                                             include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"model of degree 2 \")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 2 with regularization strength of 0\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.001)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 2 with regularization strength of 0.001\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.01)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 2 with regularization strength of 0.01\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=1)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 2 with regularization strength of 1\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for degree N=16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"model of degree 16 \")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.001)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0.001\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=0.01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=0.01)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 0.01\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alpha=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "ridge_regression = Ridge(alpha=1)\n",
    "pipeline_ridge = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"ridge_regression\", ridge_regression)])\n",
    "pipeline_ridge.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline_ridge.predict(X_test[:, np.newaxis]), label=\"model of degree 16 with regularization strength of 1\")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For an order of polynomial of one, the model of degree one is definitely not good to approximate the true function because of the shape of the true function (not a linear function). This type of function doesn't explain well the data : the model is too simple to use it. If now we try to use a degree of 2, the true function is well approximated by the model for a small value of alpha, the regularization strength. The curve corresponding well to the data when the regularization strength is small. When the regularization strength increases, we see that for N=2, the curve is smoother because it passes to fewer points. For a degree polynomial of 16, the model is really complex but passes exactly to all the points. And as before, when the regularization strength increases, the more the model curve will be smooth, so the model will be less complicated but the datas are not well approximated. \n",
    "\n",
    "The more the regularization strength is high, the more the model compelixity is reduced, because regularization penalizes weights that are too high in the model. The more the degree of the model is big, the more the model complexitity will be and the less the curve will be smooth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 What do you observe if you change the number of samples from the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's say we want 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "X_test = np.linspace(0, 1, 100) # some grid for us on the x axis\n",
    "n_samples = 100 # the number of points we will sample from true_function\n",
    "degrees = [1, 2, 16] # the polynomial degrees we will test\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_function(X) + np.random.randn(n_samples) * 0.1 # add some scaled random noise\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"model of degree 16 \")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "X_test = np.linspace(0, 1, 100) # some grid for us on the x axis\n",
    "n_samples = 10 # the number of points we will sample from true_function\n",
    "degrees = [1, 2, 16] # the polynomial degrees we will test\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_function(X) + np.random.randn(n_samples) * 0.1 # add some scaled random noise\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=16,\n",
    "                                             include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                     (\"linear_regression\", linear_regression)])\n",
    "pipeline.fit(X[:, np.newaxis], y)\n",
    "plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"model of degree 16 \")\n",
    "plt.scatter(X, y, label='noisy samples')\n",
    "plt.plot(X_test, true_function(X_test), c='r', label='true function')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we can compare the two graphs for N=16 for the case with the number of samples equals to 10 and equals to 100. We see that at 10 samples, the model doesn't fit the true function whereas at 100 samples, it is. Therefore, if we increase the number of samples, let's say 100 samples, the model will give us a better approximation to the true function, we will get a better fit as we can observe by comparision with when we had 10 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 Why do we need a test set in machine learning? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test set in machine learning check the perfomances of the model, detect and reajust the model. We need it to evaluate the model. It provides a way to assess how well the model generalizes to new, unseen data. This evaluation gives a better estimate of how the model will perform in the real world, where it encounters new inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 If we need to optimize hyperparameters, do we use the test set to select the best hyperparameters? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we use cross-valition method to get a score for each set of hyperparameters. We don't have access to it during the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
